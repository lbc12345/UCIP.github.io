<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="UCIP: A Universal Framework for Compressed Image Super-Resolution using Dynamic Prompt">
  <meta name="keywords" content="MoE-Prompts">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>UCIP: A Universal Framework for Compressed Image Super-Resolution using Dynamic Prompt</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script type="text/javascript" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://lixinustc.github.io/projects/KVQ/">
            KVQ
          </a>
          <a class="navbar-item" href="https://renyulin-f.github.io/MoE-DiffIR.github.io/">
            MoE-DiffIR
          </a>
          
          </a>
        </div>
      </div>
    </div>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">UCIP: A Universal Framework for Compressed Image Super-Resolution using Dynamic Prompt</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=sbiY97gAAAAJ&hl=en">Xin Li</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=qHeWjNwAAAAJ&hl=en&authuser=1">Bingchen Li</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Z8PYhA4AAAAJ&hl=en">Yeying Jin</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=XZugqiwAAAAJ&hl=en">Cuiling Lan</a>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=MbVZAGQAAAAJ&hl=en">Hanxin Zhu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=en&user=234Nza8AAAAJ">Yulin Ren</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=1ayDJfsAAAAJ&hl=zh-CN">Zhibo Chen</a><sup>1</sup>
            </span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block">(ECCV 2024) </span>
        </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Science and Technology of China,</span>
            <span class="author-block"><sup>2</sup>National University of Singapore,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/lbc12345/UCIP"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1LwZiTOofyhJTZb3yILSC9mCX1gsxTdrA/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" autoplay muted loop playsinline height="100%">
            <img src="./imgs/overview.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
      </img>
      <h2 class="subtitle has-text-centered">
        UCSR Dataset
      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Competition. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">UCSR Dataset Information</h2>
        <div class="content has-text-justified">
          <p>
            We build an all-in-one benchmark dataset for the compressed image super-resolution (CSR) task by collecting the datasets with the popular 6 
            diverse traditional and learning-based codecs, including traditional codecs: 
            JPEG</sup><a href="https://dl.acm.org/doi/abs/10.1145/103085.103089">[1]</a></sup>, 
            HM</sup><a href="https://link.springer.com/content/pdf/10.1007/978-3-319-06895-4.pdf">[2]</a></sup>, 
            VTM</sup><a href="https://ieeexplore.ieee.org/abstract/document/9503377">[3]</a></sup>;
            and learning-based codecs:
            \( C_{\text{PSNR}} \)</sup><a href="https://arxiv.org/abs/2001.01568">[4]</a></sup>,
            \( C_{\text{SSIM}} \)</sup><a href="https://arxiv.org/abs/2001.01568">[4]</a></sup>,
            HIFIC</sup><a href="https://arxiv.org/abs/2006.09965">[5]</a></sup>, resulting in 23 common degradations.
            We list the detailed quality factor (QF), quantization parameter (QP) and compression mode (Mode) in the following (From left to right: poorer quality -> better quality):
            <ul>
                <li>JPEG: QF=10,20,30,40</li>
                <li>HM: QP=47,42,37,32</li>
                <li>VTM: QP=47,42,37,32</li>
                <li>\( C_{\text{PSNR}} \): Mode=1,2,3,4</li>
                <li>\( C_{\text{SSIM}} \): Mode=1,2,3,4</li>
                <li>HIFIC: Mode='low', 'med', 'high'</li>
            </ul>
            We establish our UCSR dataset based on popular high-quality dataset DF2K</sup><a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/html/Agustsson_NTIRE_2017_Challenge_CVPR_2017_paper.html">[6]</a></sup>. 
            Considering the original image as the ground-truth, we generate the compressed low-resolution image with x4 bicubic downsampling and different compression codecs.
            For evaluation, we follow the image super-resolution (ISR) problems and adopt five common benchmarks: 
            Set5</sup><a href="http://eprints.imtlucca.it/2412/1/Bevilacqua_2012.pdf">[7]</a></sup>, 
            Set14</sup><a href="https://www.researchgate.net/profile/Michael-Elad/publication/220942248_On_Single_Image_Scale-Up_Using_Sparse-Representations/links/5a64b36aaca272a1581f160c/On-Single-Image-Scale-Up-Using-Sparse-Representations.pdf">[8]</a></sup>, 
            BSD100</sup><a href="https://digitalassets.lib.berkeley.edu/techreports/ucb/text/CSD-01-1133.pdf">[9]</a></sup>, 
            Urban100</sup><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Huang_Single_Image_Super-Resolution_2015_CVPR_paper.pdf">[10]</a></sup> and 
            Manga109</sup><a href="https://link.springer.com/content/pdf/10.1007/s11042-016-4020-z.pdf">[11]</a></sup>. We compress these images with various codecs based on their x4 downsampled version.
            Our full dataset, including training and testing images, can be downloaded via this <a href="https://drive.google.com/file/d/1LwZiTOofyhJTZb3yILSC9mCX1gsxTdrA/view?usp=sharing">link</a>.
            Notice that, for the ground-truth of DF2K, please download them via <a href="http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip">DIV2K</a> (800 images) and <a href="https://cv.snu.ac.kr/research/EDSR/Flickr2K.tar">Flickr2K</a> (2650 images). 
          </p>
        </div>
        <div align="center">
            <img src="./imgs/dataset.png"
                 class="interpolation-image"
                 alt="data"/>
        </div>
      </div>
    </div>
    <!--/ Competition. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Compressed Image Super-resolution (CSR) aims to simultaneously super-resolve the compressed images and tackle the challenging hybrid distortions caused by compression. However, existing works on CSR usually focus on single compression codec, \ie, JPEG, ignoring the diverse traditional or learning-based codecs in the practical application, \eg, HEVC, VVC, HIFIC, \emph{etc}. In this work, we propose the first universal CSR framework, dubbed UCIP, with dynamic prompt learning, intending to jointly support the CSR distortions of any compression codecs/modes. Particularly, an efficient dynamic prompt strategy is proposed to mine the content/spatial-aware task-adaptive contextual information for the universal CSR task, using only a small amount of prompts with spatial size $1\times1$. To simplify contextual information mining, we introduce the novel MLP-like framework backbone for our UCIP by adapting the Active Token Mixer (ATM) to CSR tasks for the first time, where the global information modeling is only taken in horizontal and vertical directions with offset prediction. We also build an all-in-one benchmark dataset for the CSR task by collecting the datasets with the popular 6 diverse traditional and learning-based codecs, including JPEG, HEVC, VVC, HIFIC, etc., resulting in 23 common degradations. Extensive experiments have shown the consistent and excellent performance of our UCIP on universal CSR tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>




</section>

<!-- Concurrent Work. -->
<div class="section">
  <div class="container is-max-desktop content">
    <h2 class="title">Reference</h2>

    <div class="content has-text-justified">
      <p>
        [1] Wallace, G.K.: The jpeg still picture compression standard. Communications of the ACM 34(4), 30–44 (1991)
      </p>
      <p>
        [2] Sze, V., Budagavi, M. and Sullivan, G.J., 2014. High efficiency video coding (HEVC). In Integrated circuit and systems, algorithms and architectures (Vol. 39, p. 40). Berlin, Germany: Springer.
      </p>
      <p>
        [3] Bross, B., Wang, Y.K., Ye, Y., Liu, S., Chen, J., Sullivan, G.J., Ohm, J.R.: Overview of the versatile video coding (vvc) standard and its applications. IEEE Transactions on Circuits and Systems for Video Technology 31(10), 3736–3764 (2021)
      </p>
      <p>
        [4] Cheng, Z., Sun, H., Takeuchi, M. and Katto, J., 2020. Learned image compression with discretized gaussian mixture likelihoods and attention modules. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 7939-7948).
      <p>
      <p>
        [5] Mentzer, F., Toderici, G.D., Tschannen, M. and Agustsson, E., 2020. High-fidelity generative image compression. Advances in Neural Information Processing Systems, 33, pp.11913-11924.
      </p>
        [6] Agustsson, E., Timofte, R.: Ntire 2017 challenge on single image super-resolution: Dataset and study. In: Proceedings of the IEEE conference on computer vision and pattern recognition workshops. pp. 126–135 (2017)
      </p>
      <p>
        [7] Low-complexity single-image super-resolution based on nonnegative neighbor embedding.
      </p>
      <p>
        [8] Zeyde R, Elad M, Protter M. On single image scale-up using sparse-representations[C]//Curves and Surfaces: 7th International Conference, Avignon, France, June 24-30, 2010, Revised Selected Papers 7. Springer Berlin Heidelberg, 2012: 711-730.
      </p>
      <p>
        [9] Martin D, Fowlkes C, Tal D, et al. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics[C]//Proceedings eighth IEEE international conference on computer vision. ICCV 2001. IEEE, 2001, 2: 416-423.
      </p>
      <p>
        [10] Huang J B, Singh A, Ahuja N. Single image super-resolution from transformed self-exemplars[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 5197-5206.
      </p>
      <p>
        [11] Matsui Y, Ito K, Aramaki Y, et al. Sketch-based manga retrieval using manga109 dataset[J]. Multimedia tools and applications, 2017, 76: 21811-21838.
      </p>
      </div>
  </div>
</div>
<!--/ Concurrent Work. -->

</div>
</section>






<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @inproceedings{li2024ucip,
        title={UCIP: A Universal Framework for Compressed Image Super-Resolution using Dynamic Prompt},
        author={Li, Xin and Li, Bingchen and Jin, Yeying and Lan, Cuiling and Zhu, Hanxin and Ren, Yulin and Chen, Zhibo},
        booktitle={European Conference on Computer Vision},
        year={2024},
        organization={Springer}
      }
    </code></pre>
  </div>
</section>







<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2402.07220.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/lixinustc" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website adapted from the following <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
